{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc43703a",
   "metadata": {
    "id": "dc43703a"
   },
   "source": [
    "<center><img src=\"./images/logo.png\" style=\"width: 300px;\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5c944d",
   "metadata": {
    "id": "7e5c944d"
   },
   "source": [
    "# **Fruit Quality Assessment Using Deep Learning**\n",
    "This notebook demonstrates a complete workflow for classifying fresh and rotten fruits using deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5593a8f",
   "metadata": {
    "id": "d5593a8f"
   },
   "source": [
    "## **Problem Statement**\n",
    "\n",
    "A leading fruit supply company has approached **FruitScan Solutions** to develop an automated solution that can distinguish between fresh and rotten fruits in real-time using image input from their supply belt. The supplier, primarily dealing in apples, oranges, and bananas, faces challenges in ensuring consistent fruit quality, often resulting in customer complaints due to the inclusion of rotten fruits in purchased baskets. Manual sorting is slow, inconsistent, and not scalable. The company seeks to increase the speed and accuracy of their quality control process by integrating a computer vision-based system that can automate fruit classification and help streamline their distribution workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iGL-4LTyAhdo",
   "metadata": {
    "id": "iGL-4LTyAhdo"
   },
   "source": [
    "### **Importing Required Libraries**\n",
    "Essential libraries for deep learning (PyTorch, torchvision), data handling, and image processing are imported. The device (CPU or GPU) is set for computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "093c4752",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "093c4752",
    "outputId": "09418cb2-f4e1-4e70-c2a6-f9f2f2066592"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.v2 as transforms\n",
    "import torchvision.io as tv_io\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "import utils\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gHHT_OeyA4eL",
   "metadata": {
    "id": "gHHT_OeyA4eL"
   },
   "source": [
    "### **Downloading and Preparing the Dataset**\n",
    "   The fruit image dataset is downloaded using KaggleHub. The dataset contains labeled images of fresh and rotten fruits, organized into training and validation folders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02a278cc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "02a278cc",
    "outputId": "c45268e8-2e13-4650-84bb-6381c0630f6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/sriramr/fruits-fresh-and-rotten-for-classification?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3.58G/3.58G [01:29<00:00, 42.9MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /root/.cache/kagglehub/datasets/sriramr/fruits-fresh-and-rotten-for-classification/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"sriramr/fruits-fresh-and-rotten-for-classification\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hq6CW3tUBt_d",
   "metadata": {
    "id": "hq6CW3tUBt_d"
   },
   "source": [
    "### **Visualizing the Dataset**\n",
    "   A sample image from the dataset is displayed to provide a visual understanding of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd259c01",
   "metadata": {
    "id": "cd259c01"
   },
   "source": [
    "<center><img src=\"./images/fruits.png\" style=\"width: 600px;\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45Muw0huCcvH",
   "metadata": {
    "id": "45Muw0huCcvH"
   },
   "source": [
    "### **Model Selection and Initialization**\n",
    "   A pre-trained VGG16 model is loaded from torchvision. The base model's weights are frozen to leverage transfer learning, and the classifier is customized for the fruit classification task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c00fff49",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c00fff49",
    "outputId": "699e36ad-aee3-4c85-9a15-c86d06f176e4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
      "100%|██████████| 528M/528M [00:07<00:00, 78.8MB/s]\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import vgg16\n",
    "from torchvision.models import VGG16_Weights\n",
    "\n",
    "weights = VGG16_Weights.DEFAULT\n",
    "vgg_model = vgg16(weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55ea97fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "55ea97fd",
    "outputId": "f3dc3058-c0dd-455a-8e1e-4a17a70e4cb5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Freeze base model\n",
    "vgg_model.requires_grad_(False)\n",
    "next(iter(vgg_model.parameters())).requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03d17579",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "03d17579",
    "outputId": "13f3e7b7-fe79-4fd0-960b-3d3ab95bb96c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_model.classifier[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ggnBwf91Cv2x",
   "metadata": {
    "id": "ggnBwf91Cv2x"
   },
   "source": [
    "### **Model Customization**\n",
    "   The classifier part of the VGG16 model is modified to output predictions for six classes (fresh and rotten for each fruit type). Additional fully connected layers and activation functions are added.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a3c8cc3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4a3c8cc3",
    "outputId": "c187613b-4fa4-4c3f-8e06-e26caa236ee6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (1): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (2): Flatten(start_dim=1, end_dim=-1)\n",
       "  (3): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (4): Linear(in_features=4096, out_features=500, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=500, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_CLASSES = 6\n",
    "\n",
    "my_model = nn.Sequential(\n",
    "    vgg_model.features,\n",
    "    vgg_model.avgpool,\n",
    "    nn.Flatten(),\n",
    "    vgg_model.classifier[0:3],\n",
    "    nn.Linear(4096, 500),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(500, N_CLASSES)\n",
    ")\n",
    "my_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xK8mcXLTC5-8",
   "metadata": {
    "id": "xK8mcXLTC5-8"
   },
   "source": [
    "### **Setting Up Loss Function and Optimizer**\n",
    "   The cross-entropy loss function is used for multi-class classification. The Adam optimizer is initialized to update the model parameters during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "faaebb2c",
   "metadata": {
    "id": "faaebb2c"
   },
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(my_model.parameters())\n",
    "my_model = torch.compile(my_model.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "U1FLOWw3DINn",
   "metadata": {
    "id": "U1FLOWw3DINn"
   },
   "source": [
    "### **Data Transformations and Augmentation**\n",
    "   Image transformations such as resizing, rotation, and horizontal flipping are applied to augment the training data and improve model generalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3be28746",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3be28746",
    "outputId": "206b8cac-2e6d-4040-8bea-2fa5192fcdb7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassification(\n",
       "    crop_size=[224]\n",
       "    resize_size=[256]\n",
       "    mean=[0.485, 0.456, 0.406]\n",
       "    std=[0.229, 0.224, 0.225]\n",
       "    interpolation=InterpolationMode.BILINEAR\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_trans = weights.transforms()\n",
    "pre_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bad244b",
   "metadata": {
    "id": "7bad244b"
   },
   "outputs": [],
   "source": [
    "IMG_WIDTH, IMG_HEIGHT = (224, 224)\n",
    "\n",
    "random_trans = transforms.Compose([\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomResizedCrop((IMG_WIDTH, IMG_HEIGHT), scale=(.8, 1), ratio=(1, 1)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qPm7IF_sDtIf",
   "metadata": {
    "id": "qPm7IF_sDtIf"
   },
   "source": [
    "### **Custom Dataset Class**\n",
    "   A custom PyTorch Dataset class is defined to load images and labels from the dataset folders, apply transformations, and prepare data for training and validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65872aa0",
   "metadata": {
    "id": "65872aa0"
   },
   "outputs": [],
   "source": [
    "DATA_LABELS = [\"freshapples\", \"freshbanana\", \"freshoranges\", \"rottenapples\", \"rottenbanana\", \"rottenoranges\"]\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.imgs = []\n",
    "        self.labels = []\n",
    "\n",
    "        for l_idx, label in enumerate(DATA_LABELS):\n",
    "            data_paths = glob.glob(data_dir + label + '/*.png', recursive=True)\n",
    "            for path in data_paths:\n",
    "                img = tv_io.read_image(path, tv_io.ImageReadMode.RGB)\n",
    "                self.imgs.append(pre_trans(img).to(device))\n",
    "                self.labels.append(torch.tensor(l_idx).to(device))\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.imgs[idx]\n",
    "        label = self.labels[idx]\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5lwLCPk0D_oO",
   "metadata": {
    "id": "5lwLCPk0D_oO"
   },
   "source": [
    "### **DataLoader Setup**\n",
    "   PyTorch DataLoaders are created for both training and validation datasets to efficiently batch and shuffle data during model training and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3KqO9UgOEFkZ",
   "metadata": {
    "id": "3KqO9UgOEFkZ"
   },
   "outputs": [],
   "source": [
    "n = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "RJV9w4ul4CXL",
   "metadata": {
    "id": "RJV9w4ul4CXL"
   },
   "outputs": [],
   "source": [
    "train_path = os.path.join(path,'dataset/train/')\n",
    "train_data = MyDataset(train_path)\n",
    "train_loader = DataLoader(train_data, batch_size=n, shuffle=True)\n",
    "train_N = len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "LgC5-8LK-15z",
   "metadata": {
    "id": "LgC5-8LK-15z"
   },
   "outputs": [],
   "source": [
    "valid_path = os.path.join(path, 'dataset/test/')\n",
    "valid_data = MyDataset(valid_path)\n",
    "valid_loader = DataLoader(valid_data, batch_size=n, shuffle=False)\n",
    "valid_N = len(valid_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2yOYBS04-kUf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2yOYBS04-kUf",
    "outputId": "2186e8e4-4887-4580-ee9a-1bb28543a322"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10901, 2698)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_N, valid_N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eN-mecuTEh3q",
   "metadata": {
    "id": "eN-mecuTEh3q"
   },
   "source": [
    "### **Model Training**\n",
    "   The model is trained for several epochs using the training data. After each epoch, the model's performance is validated on the validation set to monitor progress and prevent overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d14ec110",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d14ec110",
    "outputId": "80dc61db-f2e0-47b8-bbcd-2ef57a8f749f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0507 19:34:32.644000 370 torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 52.1945 Accuracy: 0.9473\n",
      "Valid - Loss: 6.4263 Accuracy: 0.9722\n",
      "Epoch: 2\n",
      "Train - Loss: 26.3110 Accuracy: 0.9719\n",
      "Valid - Loss: 5.0178 Accuracy: 0.9804\n",
      "Epoch: 3\n",
      "Train - Loss: 22.5769 Accuracy: 0.9761\n",
      "Valid - Loss: 2.6496 Accuracy: 0.9900\n",
      "Epoch: 4\n",
      "Train - Loss: 14.5521 Accuracy: 0.9850\n",
      "Valid - Loss: 4.9524 Accuracy: 0.9833\n",
      "Epoch: 5\n",
      "Train - Loss: 13.5720 Accuracy: 0.9855\n",
      "Valid - Loss: 3.8268 Accuracy: 0.9896\n",
      "Epoch: 6\n",
      "Train - Loss: 14.5329 Accuracy: 0.9847\n",
      "Valid - Loss: 3.0160 Accuracy: 0.9863\n",
      "Epoch: 7\n",
      "Train - Loss: 10.8092 Accuracy: 0.9895\n",
      "Valid - Loss: 3.2797 Accuracy: 0.9885\n",
      "Epoch: 8\n",
      "Train - Loss: 12.0271 Accuracy: 0.9883\n",
      "Valid - Loss: 1.4801 Accuracy: 0.9930\n",
      "Epoch: 9\n",
      "Train - Loss: 12.7056 Accuracy: 0.9872\n",
      "Valid - Loss: 5.4825 Accuracy: 0.9822\n",
      "Epoch: 10\n",
      "Train - Loss: 10.5114 Accuracy: 0.9903\n",
      "Valid - Loss: 3.8088 Accuracy: 0.9889\n",
      "Epoch: 11\n",
      "Train - Loss: 7.8565 Accuracy: 0.9919\n",
      "Valid - Loss: 1.8540 Accuracy: 0.9926\n",
      "Epoch: 12\n",
      "Train - Loss: 8.3065 Accuracy: 0.9922\n",
      "Valid - Loss: 3.1216 Accuracy: 0.9900\n",
      "Epoch: 13\n",
      "Train - Loss: 9.1772 Accuracy: 0.9909\n",
      "Valid - Loss: 2.7767 Accuracy: 0.9915\n",
      "Epoch: 14\n",
      "Train - Loss: 6.9239 Accuracy: 0.9932\n",
      "Valid - Loss: 5.9366 Accuracy: 0.9859\n",
      "Epoch: 15\n",
      "Train - Loss: 8.8016 Accuracy: 0.9926\n",
      "Valid - Loss: 4.3337 Accuracy: 0.9907\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Epoch: {}'.format(epoch+1))\n",
    "    utils.train(my_model, train_loader, train_N, random_trans, optimizer, loss_function)\n",
    "    utils.validate(my_model, valid_loader, valid_N, loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hDNprYIbE0h7",
   "metadata": {
    "id": "hDNprYIbE0h7"
   },
   "source": [
    "### **Fine-Tuning the Model**\n",
    "   After initial training, the base VGG16 model is unfrozen to allow fine-tuning of all layers. The optimizer's learning rate is reduced, and the model is trained for additional epochs to further improve accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17c8a6ad",
   "metadata": {
    "id": "17c8a6ad"
   },
   "outputs": [],
   "source": [
    "# Unfreeze the base model\n",
    "vgg_model.requires_grad_(True)\n",
    "optimizer = Adam(my_model.parameters(), lr=.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a356ea5d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a356ea5d",
    "outputId": "d61c13da-6bfd-43ce-cf4c-8eb172f18aae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train - Loss: 3.3145 Accuracy: 0.9972\n",
      "Valid - Loss: 2.3053 Accuracy: 0.9941\n",
      "Epoch: 2\n",
      "Train - Loss: 2.9193 Accuracy: 0.9978\n",
      "Valid - Loss: 2.3634 Accuracy: 0.9937\n",
      "Epoch: 3\n",
      "Train - Loss: 3.1447 Accuracy: 0.9962\n",
      "Valid - Loss: 1.4405 Accuracy: 0.9944\n",
      "Epoch: 4\n",
      "Train - Loss: 2.9512 Accuracy: 0.9971\n",
      "Valid - Loss: 1.3282 Accuracy: 0.9956\n",
      "Epoch: 5\n",
      "Train - Loss: 2.3268 Accuracy: 0.9978\n",
      "Valid - Loss: 1.0168 Accuracy: 0.9956\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Epoch: {}'.format(epoch+1))\n",
    "    utils.train(my_model, train_loader, train_N, random_trans, optimizer, loss_function)\n",
    "    utils.validate(my_model, valid_loader, valid_N, loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oUQiE0j4FLA2",
   "metadata": {
    "id": "oUQiE0j4FLA2"
   },
   "source": [
    "### **Model Evaluation**\n",
    "   The final trained model is evaluated on the validation set to assess its classification performance. Metrics such as accuracy and loss are reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aaa6a14b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aaa6a14b",
    "outputId": "750a8ee0-8cbb-4ab5-ee83-c9bb2aa12eb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid - Loss: 1.0168 Accuracy: 0.9956\n"
     ]
    }
   ],
   "source": [
    "utils.validate(my_model, valid_loader, valid_N, loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816f99e7",
   "metadata": {},
   "source": [
    "### **Testing the Model on New Images**\n",
    "The following function allows you to test the trained fruit classification model on any new image. It loads an image, applies the necessary preprocessing, and predicts whether the fruit is fresh or rotten. You can use this function to evaluate the model's performance on unseen data or your own images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "MLQLyJ7IMwaB",
   "metadata": {
    "id": "MLQLyJ7IMwaB"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from PIL import Image\n",
    "\n",
    "def test_model(model, image_path):\n",
    "\n",
    "    # Load and preprocess the image\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    pre_trans = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    img_tensor = pre_trans(img).unsqueeze(0).to(device)\n",
    "\n",
    "    # Make a prediction\n",
    "    with torch.no_grad():\n",
    "      model.eval()\n",
    "      output = model(img_tensor)\n",
    "\n",
    "    # Get the predicted class\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    predicted_class = DATA_LABELS[predicted.item()]\n",
    "\n",
    "    print(f\"Predicted class for {image_path}: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "W9ofen2qWPyB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W9ofen2qWPyB",
    "outputId": "d3669365-2356-4ab3-a01e-be528e514654"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class for /content/images/pic2.png: rottenbanana\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "# image_path = \"/content/images/pic1.png\"\n",
    "image_path = \"/content/images/pic2.png\"\n",
    "# image_path = \"/content/images/pic3.png\"\n",
    "test_model(my_model, image_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RNi9L-PdFX2V",
   "metadata": {
    "id": "RNi9L-PdFX2V"
   },
   "source": [
    "## **Saving the Model**\n",
    "Finally the Fine-tuned model is saved in the `model` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "JR8fyzCA_zTo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JR8fyzCA_zTo",
    "outputId": "46b88a91-0d4f-45ca-e5dc-1f16866536a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully in the model folder.\n"
     ]
    }
   ],
   "source": [
    "# Create the directory if it doesn't exist\n",
    "!mkdir -p model\n",
    "\n",
    "# Save the model\n",
    "torch.save(my_model, 'model/fruit_classification_model.pth')\n",
    "print(\"Model saved successfully in the model folder.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
