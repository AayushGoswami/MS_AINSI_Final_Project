{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc43703a",
   "metadata": {},
   "source": [
    "<center><img src=\"./images/logo.png\" style=\"width: 300px;\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5c944d",
   "metadata": {
    "id": "7e5c944d"
   },
   "source": [
    "# Fruit Classification Notebook Workflow\n",
    "This notebook demonstrates a complete workflow for classifying fresh and rotten fruits using deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5593a8f",
   "metadata": {
    "id": "d5593a8f"
   },
   "source": [
    "#### Problem Statement\n",
    "\n",
    "A leading fruit supply company has approached **FruitScan Solutions** to develop an automated solution that can distinguish between fresh and rotten fruits in real-time using image input from their supply belt. The supplier, primarily dealing in apples, oranges, and bananas, faces challenges in ensuring consistent fruit quality, often resulting in customer complaints due to the inclusion of rotten fruits in purchased baskets. Manual sorting is slow, inconsistent, and not scalable. The company seeks to increase the speed and accuracy of their quality control process by integrating a computer vision-based system that can automate fruit classification and help streamline their distribution workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iGL-4LTyAhdo",
   "metadata": {
    "id": "iGL-4LTyAhdo"
   },
   "source": [
    "## Importing Required Libraries\n",
    "Essential libraries for deep learning (PyTorch, torchvision), data handling, and image processing are imported. The device (CPU or GPU) is set for computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "093c4752",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "093c4752",
    "outputId": "5b7eebc8-6c3f-4cdd-f08e-91823bdb3db8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.v2 as transforms\n",
    "import torchvision.io as tv_io\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "import utils\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gHHT_OeyA4eL",
   "metadata": {
    "id": "gHHT_OeyA4eL"
   },
   "source": [
    "## Downloading and Preparing the Dataset\n",
    "   The fruit image dataset is downloaded using KaggleHub. The dataset contains labeled images of fresh and rotten fruits, organized into training and validation folders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02a278cc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "02a278cc",
    "outputId": "6c177cfd-272b-4f32-bc06-3029a7258ebf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /kaggle/input/fruits-fresh-and-rotten-for-classification\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"sriramr/fruits-fresh-and-rotten-for-classification\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hq6CW3tUBt_d",
   "metadata": {
    "id": "hq6CW3tUBt_d"
   },
   "source": [
    "## Visualizing the Dataset\n",
    "   A sample image from the dataset is displayed to provide a visual understanding of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd259c01",
   "metadata": {
    "id": "cd259c01"
   },
   "source": [
    "<center><img src=\"./images/fruits.png\" style=\"width: 600px;\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45Muw0huCcvH",
   "metadata": {
    "id": "45Muw0huCcvH"
   },
   "source": [
    "## Model Selection and Initialization\n",
    "   A pre-trained VGG16 model is loaded from torchvision. The base model's weights are frozen to leverage transfer learning, and the classifier is customized for the fruit classification task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c00fff49",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c00fff49",
    "outputId": "34d16359-aa20-4e04-a313-91de65a83cd7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
      "100%|██████████| 528M/528M [00:07<00:00, 77.6MB/s]\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import vgg16\n",
    "from torchvision.models import VGG16_Weights\n",
    "\n",
    "weights = VGG16_Weights.DEFAULT\n",
    "vgg_model = vgg16(weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55ea97fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "55ea97fd",
    "outputId": "993f73d2-dd05-408a-ee73-c234ff273036"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Freeze base model\n",
    "vgg_model.requires_grad_(False)\n",
    "next(iter(vgg_model.parameters())).requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03d17579",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "03d17579",
    "outputId": "10dc1398-49d8-4346-83d8-f2f5a856cf77"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_model.classifier[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ggnBwf91Cv2x",
   "metadata": {
    "id": "ggnBwf91Cv2x"
   },
   "source": [
    "## Model Customization\n",
    "   The classifier part of the VGG16 model is modified to output predictions for six classes (fresh and rotten for each fruit type). Additional fully connected layers and activation functions are added.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a3c8cc3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4a3c8cc3",
    "outputId": "e602a638-0d5a-43b5-93f3-1fbd40474162"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (1): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (2): Flatten(start_dim=1, end_dim=-1)\n",
       "  (3): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (4): Linear(in_features=4096, out_features=500, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=500, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_CLASSES = 6\n",
    "\n",
    "my_model = nn.Sequential(\n",
    "    vgg_model.features,\n",
    "    vgg_model.avgpool,\n",
    "    nn.Flatten(),\n",
    "    vgg_model.classifier[0:3],\n",
    "    nn.Linear(4096, 500),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(500, N_CLASSES)\n",
    ")\n",
    "my_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xK8mcXLTC5-8",
   "metadata": {
    "id": "xK8mcXLTC5-8"
   },
   "source": [
    "## Setting Up Loss Function and Optimizer\n",
    "   The cross-entropy loss function is used for multi-class classification. The Adam optimizer is initialized to update the model parameters during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "faaebb2c",
   "metadata": {
    "id": "faaebb2c"
   },
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(my_model.parameters())\n",
    "my_model = torch.compile(my_model.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "U1FLOWw3DINn",
   "metadata": {
    "id": "U1FLOWw3DINn"
   },
   "source": [
    "## Data Transformations and Augmentation\n",
    "   Image transformations such as resizing, rotation, and horizontal flipping are applied to augment the training data and improve model generalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3be28746",
   "metadata": {
    "id": "3be28746"
   },
   "outputs": [],
   "source": [
    "pre_trans = weights.transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bad244b",
   "metadata": {
    "id": "7bad244b"
   },
   "outputs": [],
   "source": [
    "IMG_WIDTH, IMG_HEIGHT = (224, 224)\n",
    "\n",
    "random_trans = transforms.Compose([\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomResizedCrop((IMG_WIDTH, IMG_HEIGHT), scale=(.8, 1), ratio=(1, 1)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qPm7IF_sDtIf",
   "metadata": {
    "id": "qPm7IF_sDtIf"
   },
   "source": [
    "## Custom Dataset Class\n",
    "   A custom PyTorch Dataset class is defined to load images and labels from the dataset folders, apply transformations, and prepare data for training and validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65872aa0",
   "metadata": {
    "id": "65872aa0"
   },
   "outputs": [],
   "source": [
    "DATA_LABELS = [\"freshapples\", \"freshbanana\", \"freshoranges\", \"rottenapples\", \"rottenbanana\", \"rottenoranges\"]\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.imgs = []\n",
    "        self.labels = []\n",
    "\n",
    "        for l_idx, label in enumerate(DATA_LABELS):\n",
    "            data_paths = glob.glob(data_dir + label + '/*.png', recursive=True)\n",
    "            for path in data_paths:\n",
    "                img = tv_io.read_image(path, tv_io.ImageReadMode.RGB)\n",
    "                self.imgs.append(pre_trans(img).to(device))\n",
    "                self.labels.append(torch.tensor(l_idx).to(device))\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.imgs[idx]\n",
    "        label = self.labels[idx]\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5lwLCPk0D_oO",
   "metadata": {
    "id": "5lwLCPk0D_oO"
   },
   "source": [
    "## DataLoader Setup\n",
    "   PyTorch DataLoaders are created for both training and validation datasets to efficiently batch and shuffle data during model training and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3KqO9UgOEFkZ",
   "metadata": {
    "id": "3KqO9UgOEFkZ"
   },
   "outputs": [],
   "source": [
    "n = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "RJV9w4ul4CXL",
   "metadata": {
    "id": "RJV9w4ul4CXL"
   },
   "outputs": [],
   "source": [
    "train_path = '/kaggle/input/fruits-fresh-and-rotten-for-classification/dataset/train/'\n",
    "train_data = MyDataset(train_path)\n",
    "train_loader = DataLoader(train_data, batch_size=n, shuffle=True)\n",
    "train_N = len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "LgC5-8LK-15z",
   "metadata": {
    "id": "LgC5-8LK-15z"
   },
   "outputs": [],
   "source": [
    "valid_path = '/kaggle/input/fruits-fresh-and-rotten-for-classification/dataset/test/'\n",
    "valid_data = MyDataset(valid_path)\n",
    "valid_loader = DataLoader(valid_data, batch_size=n, shuffle=False)\n",
    "valid_N = len(valid_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2yOYBS04-kUf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2yOYBS04-kUf",
    "outputId": "a9dd684e-ef01-4cef-84d5-eeb55b20d850"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10901, 2698)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_N, valid_N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eN-mecuTEh3q",
   "metadata": {
    "id": "eN-mecuTEh3q"
   },
   "source": [
    "## Model Training\n",
    "   The model is trained for several epochs using the training data. After each epoch, the model's performance is validated on the validation set to monitor progress and prevent overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d14ec110",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d14ec110",
    "outputId": "a664227a-713f-4845-bfbc-fe70e5ef27a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Train - Loss: 21.9565 Accuracy: 0.9764\n",
      "Valid - Loss: 5.1158 Accuracy: 0.9822\n",
      "Epoch: 1\n",
      "Train - Loss: 18.1268 Accuracy: 0.9824\n",
      "Valid - Loss: 4.0402 Accuracy: 0.9811\n",
      "Epoch: 2\n",
      "Train - Loss: 13.0671 Accuracy: 0.9861\n",
      "Valid - Loss: 2.1111 Accuracy: 0.9900\n",
      "Epoch: 3\n",
      "Train - Loss: 13.0427 Accuracy: 0.9880\n",
      "Valid - Loss: 2.6491 Accuracy: 0.9911\n",
      "Epoch: 4\n",
      "Train - Loss: 11.4938 Accuracy: 0.9871\n",
      "Valid - Loss: 1.7292 Accuracy: 0.9937\n",
      "Epoch: 5\n",
      "Train - Loss: 10.0478 Accuracy: 0.9906\n",
      "Valid - Loss: 1.9499 Accuracy: 0.9922\n",
      "Epoch: 6\n",
      "Train - Loss: 13.7807 Accuracy: 0.9859\n",
      "Valid - Loss: 3.1426 Accuracy: 0.9870\n",
      "Epoch: 7\n",
      "Train - Loss: 8.0254 Accuracy: 0.9918\n",
      "Valid - Loss: 2.4182 Accuracy: 0.9896\n",
      "Epoch: 8\n",
      "Train - Loss: 9.2837 Accuracy: 0.9907\n",
      "Valid - Loss: 2.3325 Accuracy: 0.9904\n",
      "Epoch: 9\n",
      "Train - Loss: 7.5666 Accuracy: 0.9932\n",
      "Valid - Loss: 2.2021 Accuracy: 0.9900\n",
      "Epoch: 10\n",
      "Train - Loss: 7.4638 Accuracy: 0.9923\n",
      "Valid - Loss: 1.5260 Accuracy: 0.9937\n",
      "Epoch: 11\n",
      "Train - Loss: 7.7227 Accuracy: 0.9929\n",
      "Valid - Loss: 2.9750 Accuracy: 0.9896\n",
      "Epoch: 12\n",
      "Train - Loss: 9.3550 Accuracy: 0.9924\n",
      "Valid - Loss: 3.9776 Accuracy: 0.9852\n",
      "Epoch: 13\n",
      "Train - Loss: 11.8207 Accuracy: 0.9919\n",
      "Valid - Loss: 2.7397 Accuracy: 0.9900\n",
      "Epoch: 14\n",
      "Train - Loss: 7.5060 Accuracy: 0.9928\n",
      "Valid - Loss: 2.4995 Accuracy: 0.9926\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Epoch: {}'.format(epoch))\n",
    "    utils.train(my_model, train_loader, train_N, random_trans, optimizer, loss_function)\n",
    "    utils.validate(my_model, valid_loader, valid_N, loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hDNprYIbE0h7",
   "metadata": {
    "id": "hDNprYIbE0h7"
   },
   "source": [
    "## Fine-Tuning the Model\n",
    "   After initial training, the base VGG16 model is unfrozen to allow fine-tuning of all layers. The optimizer's learning rate is reduced, and the model is trained for additional epochs to further improve accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17c8a6ad",
   "metadata": {
    "id": "17c8a6ad"
   },
   "outputs": [],
   "source": [
    "# Unfreeze the base model\n",
    "vgg_model.requires_grad_(True)\n",
    "optimizer = Adam(my_model.parameters(), lr=.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a356ea5d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a356ea5d",
    "outputId": "59cb8b46-1250-4a03-caa4-f42d5ceb07f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Train - Loss: 2.6605 Accuracy: 0.9972\n",
      "Valid - Loss: 1.7653 Accuracy: 0.9944\n",
      "Epoch: 1\n",
      "Train - Loss: 3.1402 Accuracy: 0.9975\n",
      "Valid - Loss: 1.5938 Accuracy: 0.9948\n",
      "Epoch: 2\n",
      "Train - Loss: 2.3308 Accuracy: 0.9972\n",
      "Valid - Loss: 1.2963 Accuracy: 0.9948\n",
      "Epoch: 3\n",
      "Train - Loss: 2.5016 Accuracy: 0.9974\n",
      "Valid - Loss: 1.4688 Accuracy: 0.9956\n",
      "Epoch: 4\n",
      "Train - Loss: 2.2837 Accuracy: 0.9976\n",
      "Valid - Loss: 1.3724 Accuracy: 0.9956\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Epoch: {}'.format(epoch))\n",
    "    utils.train(my_model, train_loader, train_N, random_trans, optimizer, loss_function)\n",
    "    utils.validate(my_model, valid_loader, valid_N, loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oUQiE0j4FLA2",
   "metadata": {
    "id": "oUQiE0j4FLA2"
   },
   "source": [
    "## Model Evaluation\n",
    "   The final trained model is evaluated on the validation set to assess its classification performance. Metrics such as accuracy and loss are reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aaa6a14b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aaa6a14b",
    "outputId": "b0cb637c-8d2e-4be7-ff3a-58757017951e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid - Loss: 1.3724 Accuracy: 0.9956\n"
     ]
    }
   ],
   "source": [
    "utils.validate(my_model, valid_loader, valid_N, loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RNi9L-PdFX2V",
   "metadata": {
    "id": "RNi9L-PdFX2V"
   },
   "source": [
    "## Saving the Model\n",
    "FInally the FIne-tuned model is saved in the `model` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JR8fyzCA_zTo",
   "metadata": {
    "id": "JR8fyzCA_zTo"
   },
   "outputs": [],
   "source": [
    "# # Create the directory if it doesn't exist\n",
    "# !mkdir -p model\n",
    "\n",
    "# # Save the model\n",
    "# torch.save(my_model.state_dict(), 'model/fruit_classification_model.pth')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
